{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pickle\n",
                "from tensorflow.keras.models import load_model\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "\n",
                "# Load resources\n",
                "with open('../data/tokenizer.pkl', 'rb') as f:\n",
                "    tokenizer = pickle.load(f)\n",
                "\n",
                "# Load the model (Change filename to 'deep_rnn_model.keras' if you want to use that one)\n",
                "model_path = '../saved_models/simple_rnn_model.h5'\n",
                "# model_path = '../data/deep_rnn_model.keras'\n",
                "\n",
                "try:\n",
                "    model = load_model(model_path)\n",
                "    print(f\"Model loaded from {model_path}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading model: {e}\")\n",
                "    print(\"Make sure you have trained the model first!\")\n",
                "\n",
                "max_sequence_length = model.input_shape[1]\n",
                "print(f\"Max sequence length: {max_sequence_length}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Generate Text**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_length):\n",
                "    for _ in range(next_words):\n",
                "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
                "        token_list = pad_sequences([token_list], maxlen=max_sequence_length, padding='pre')\n",
                "        \n",
                "        predicted_probs = model.predict(token_list, verbose=0)\n",
                "        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n",
                "        \n",
                "        output_word = \"\"\n",
                "        for word, index in tokenizer.word_index.items():\n",
                "            if index == predicted_index:\n",
                "                output_word = word\n",
                "                break\n",
                "        \n",
                "        seed_text += \" \" + output_word\n",
                "    return seed_text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Prediction\n",
                "seed_text = \"the quick brown fox\"\n",
                "next_words = 10\n",
                "\n",
                "generated_text = generate_text(seed_text, next_words, model, tokenizer, max_sequence_length)\n",
                "print(\"Generated Text:\")\n",
                "print(generated_text)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
